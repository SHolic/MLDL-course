{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CPSC-5616EL: Gridworld - Q-Learning (fixed path)\n",
        "\n",
        "\n",
        "The current implementation is based on Q-learning algorithm but only apply to a fixed policy (a fixed path) = [(3, 1), (2, 1), (1, 1), (0, 1)] to demostrate the convergence of the Q-values which is capped to 20 iterations (episods) for the terminal state (0,1)."
      ],
      "metadata": {
        "id": "SDscE5n1Kxmv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dIti3AXYKtNo",
        "outputId": "cf5986c3-fc3f-40b2-a26d-9abd28efc815"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1\n",
            "Q-values for the path:\n",
            "State (3,1): (1, 0) Q=0.00\n",
            "State (2,1): (1, 0) Q=0.00\n",
            "State (1,1): (1, 0) Q=0.00\n",
            "State (0,1): (-1, 0) Q=10.00\n",
            "-----------------------------\n",
            "Iteration 2\n",
            "Q-values for the path:\n",
            "State (3,1): (1, 0) Q=0.00\n",
            "State (2,1): (1, 0) Q=0.00\n",
            "State (1,1): (-1, 0) Q=4.25\n",
            "State (0,1): (-1, 0) Q=10.00\n",
            "-----------------------------\n",
            "Iteration 3\n",
            "Q-values for the path:\n",
            "State (3,1): (1, 0) Q=0.00\n",
            "State (2,1): (-1, 0) Q=1.25\n",
            "State (1,1): (-1, 0) Q=6.62\n",
            "State (0,1): (-1, 0) Q=10.00\n",
            "-----------------------------\n",
            "Iteration 4\n",
            "Q-values for the path:\n",
            "State (3,1): (1, 0) Q=0.00\n",
            "State (2,1): (-1, 0) Q=3.44\n",
            "State (1,1): (-1, 0) Q=7.81\n",
            "State (0,1): (-1, 0) Q=10.00\n",
            "-----------------------------\n",
            "Iteration 5\n",
            "Q-values for the path:\n",
            "State (3,1): (-1, 0) Q=1.06\n",
            "State (2,1): (-1, 0) Q=5.12\n",
            "State (1,1): (-1, 0) Q=8.41\n",
            "State (0,1): (-1, 0) Q=10.00\n",
            "-----------------------------\n",
            "Iteration 6\n",
            "Q-values for the path:\n",
            "State (3,1): (-1, 0) Q=2.59\n",
            "State (2,1): (-1, 0) Q=6.27\n",
            "State (1,1): (-1, 0) Q=8.70\n",
            "State (0,1): (-1, 0) Q=10.00\n",
            "-----------------------------\n",
            "Iteration 7\n",
            "Q-values for the path:\n",
            "State (3,1): (-1, 0) Q=3.93\n",
            "State (2,1): (-1, 0) Q=6.98\n",
            "State (1,1): (-1, 0) Q=8.85\n",
            "State (0,1): (-1, 0) Q=10.00\n",
            "-----------------------------\n",
            "Iteration 8\n",
            "Q-values for the path:\n",
            "State (3,1): (-1, 0) Q=4.96\n",
            "State (2,1): (-1, 0) Q=7.42\n",
            "State (1,1): (-1, 0) Q=8.93\n",
            "State (0,1): (-1, 0) Q=10.00\n",
            "-----------------------------\n",
            "Iteration 9\n",
            "Q-values for the path:\n",
            "State (3,1): (-1, 0) Q=5.69\n",
            "State (2,1): (-1, 0) Q=7.67\n",
            "State (1,1): (-1, 0) Q=8.96\n",
            "State (0,1): (-1, 0) Q=10.00\n",
            "-----------------------------\n",
            "Iteration 10\n",
            "Q-values for the path:\n",
            "State (3,1): (-1, 0) Q=6.18\n",
            "State (2,1): (-1, 0) Q=7.82\n",
            "State (1,1): (-1, 0) Q=8.98\n",
            "State (0,1): (-1, 0) Q=10.00\n",
            "-----------------------------\n",
            "Iteration 11\n",
            "Q-values for the path:\n",
            "State (3,1): (-1, 0) Q=6.50\n",
            "State (2,1): (-1, 0) Q=7.90\n",
            "State (1,1): (-1, 0) Q=8.99\n",
            "State (0,1): (-1, 0) Q=10.00\n",
            "-----------------------------\n",
            "Iteration 12\n",
            "Q-values for the path:\n",
            "State (3,1): (-1, 0) Q=6.70\n",
            "State (2,1): (-1, 0) Q=7.95\n",
            "State (1,1): (-1, 0) Q=9.00\n",
            "State (0,1): (-1, 0) Q=10.00\n",
            "-----------------------------\n",
            "Iteration 13\n",
            "Q-values for the path:\n",
            "State (3,1): (-1, 0) Q=6.82\n",
            "State (2,1): (-1, 0) Q=7.97\n",
            "State (1,1): (-1, 0) Q=9.00\n",
            "State (0,1): (-1, 0) Q=10.00\n",
            "-----------------------------\n",
            "Iteration 14\n",
            "Q-values for the path:\n",
            "State (3,1): (-1, 0) Q=6.90\n",
            "State (2,1): (-1, 0) Q=7.98\n",
            "State (1,1): (-1, 0) Q=9.00\n",
            "State (0,1): (-1, 0) Q=10.00\n",
            "-----------------------------\n",
            "Iteration 15\n",
            "Q-values for the path:\n",
            "State (3,1): (-1, 0) Q=6.94\n",
            "State (2,1): (-1, 0) Q=7.99\n",
            "State (1,1): (-1, 0) Q=9.00\n",
            "State (0,1): (-1, 0) Q=10.00\n",
            "-----------------------------\n",
            "Iteration 16\n",
            "Q-values for the path:\n",
            "State (3,1): (-1, 0) Q=6.97\n",
            "State (2,1): (-1, 0) Q=8.00\n",
            "State (1,1): (-1, 0) Q=9.00\n",
            "State (0,1): (-1, 0) Q=10.00\n",
            "-----------------------------\n",
            "Iteration 17\n",
            "Q-values for the path:\n",
            "State (3,1): (-1, 0) Q=6.98\n",
            "State (2,1): (-1, 0) Q=8.00\n",
            "State (1,1): (-1, 0) Q=9.00\n",
            "State (0,1): (-1, 0) Q=10.00\n",
            "-----------------------------\n",
            "Iteration 18\n",
            "Q-values for the path:\n",
            "State (3,1): (-1, 0) Q=6.99\n",
            "State (2,1): (-1, 0) Q=8.00\n",
            "State (1,1): (-1, 0) Q=9.00\n",
            "State (0,1): (-1, 0) Q=10.00\n",
            "-----------------------------\n",
            "Iteration 19\n",
            "Q-values for the path:\n",
            "State (3,1): (-1, 0) Q=6.99\n",
            "State (2,1): (-1, 0) Q=8.00\n",
            "State (1,1): (-1, 0) Q=9.00\n",
            "State (0,1): (-1, 0) Q=10.00\n",
            "-----------------------------\n",
            "Iteration 20\n",
            "Q-values for the path:\n",
            "State (3,1): (-1, 0) Q=7.00\n",
            "State (2,1): (-1, 0) Q=8.00\n",
            "State (1,1): (-1, 0) Q=9.00\n",
            "State (0,1): (-1, 0) Q=10.00\n",
            "-----------------------------\n",
            "Final Q-values for the path:\n",
            "State (3,1): (-1, 0) Q=7.00\n",
            "State (2,1): (-1, 0) Q=8.00\n",
            "State (1,1): (-1, 0) Q=9.00\n",
            "State (0,1): (-1, 0) Q=10.00\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Grid size\n",
        "n, m = 4, 3\n",
        "\n",
        "# Rewards grid setup with living reward -1\n",
        "rewards = np.array([\n",
        "    [-1, 10, -10],\n",
        "    [-1, -1, -1],\n",
        "    [-1, -1, -1],\n",
        "    [-1, -1, -1]\n",
        "])\n",
        "\n",
        "# Initialize Q-values to zeros (Q-table)\n",
        "q_values = np.zeros((n, m, 4))\n",
        "\n",
        "# Possible actions that the agent can take: move up, down, left, or right.\n",
        "actions = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n",
        "\n",
        "# Discount factor gamma determines the agent's consideration for future rewards.\n",
        "gamma = 1.0\n",
        "\n",
        "# Learning rate alpha for Q-learning\n",
        "alpha = 0.5\n",
        "\n",
        "# Define the fixed path the agent will follow during training\n",
        "path = [(3, 1), (2, 1), (1, 1), (0, 1)]\n",
        "\n",
        "# Number of iterations\n",
        "iterations = 20\n",
        "\n",
        "for it in range(iterations):\n",
        "    print(f\"Iteration {it + 1}\")\n",
        "\n",
        "    for i, j in path:\n",
        "        action_index = 0  # Agent's action index (moving up)\n",
        "        action = actions[action_index]\n",
        "\n",
        "        # Calculate the next state (ni, nj) based on the action\n",
        "        ni, nj = i + action[0], j + action[1]\n",
        "\n",
        "        # If current state is (0,1) or (0,2), update the Q-values based on their respective rewards\n",
        "\n",
        "        if (i, j) == (0, 1):\n",
        "            q_values[i, j, action_index] = 10\n",
        "            continue\n",
        "        elif (i, j) == (0, 2):\n",
        "            q_values[i, j, action_index] = -10\n",
        "            continue\n",
        "\n",
        "        # Q-learning update rule:\n",
        "        # 1. Calculate the sample using the reward and the maximum Q-value of the next state\n",
        "        sample = rewards[i, j] + gamma * np.max(q_values[ni, nj])\n",
        "\n",
        "        # 2. Update Q-value using the Q-learning update rule (weighted average)\n",
        "        q_values[i, j, action_index] = (1 - alpha) * q_values[i, j, action_index] + alpha * sample\n",
        "\n",
        "        # Clip the Q-value to a maximum of 10\n",
        "        q_values[i, j, action_index] = min(q_values[i, j, action_index], 10.0)\n",
        "\n",
        "    # Print Q-values for the specified path in this iteration\n",
        "    print(\"Q-values for the path:\")\n",
        "    for i, j in path:\n",
        "        action_index = np.argmax(q_values[i, j])\n",
        "        max_action = actions[action_index]\n",
        "        print(f\"State ({i},{j}): {max_action} Q={q_values[i, j, action_index]:.2f}\")\n",
        "\n",
        "    print(\"-----------------------------\")\n",
        "\n",
        "# Display final Q-values for the specified path\n",
        "print(\"Final Q-values for the path:\")\n",
        "for i, j in path:\n",
        "    action_index = np.argmax(q_values[i, j])\n",
        "    max_action = actions[action_index]\n",
        "    print(f\"State ({i},{j}): {max_action} Q={q_values[i, j, action_index]:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bHf_Ay9pKw1a"
      }
    }
  ]
}