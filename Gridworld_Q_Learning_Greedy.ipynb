{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CPSC-5616EL: Gridworld - Q-Learning (with epsilon-greedy)\n",
        "\n",
        "\n",
        "This sample shows the implementation of Q-learning (a model-free reinforcement learning algorithm) applied to a simple grid world. The agent can take four possible actions at each state: move up, down, left, or right. The Q-values represent the expected cumulative reward the agent can obtain by taking a specific action in a specific state.\n",
        "\n",
        "**Q-learning with epsilon-greedy exploration:**\n",
        "\n",
        "For each state (i, j), the algorithm iterates over possible actions.\n",
        "It then calculates the resulting state (ni, nj) from taking an action.\n",
        "The agent decides whether to explore (choose a random action) or exploit (choose the action with the highest Q-value for the state) based on the epsilon value.\n",
        "Q-value for the current state and action is updated using the Q-learning update rule.\n",
        "\n",
        "**Q-value Clipping:**\n",
        "\n",
        "After each Q-value update, the code ensures that the Q-value for the action at state (0,1) is at most 10, and the Q-value for the action at state (0,2) is at least -10.\n",
        "\n",
        "The Q-values represent the agent's learned values for how good each action is for each state, considering future rewards.\n",
        "\n",
        "When you run this code, it will give you the Q-values after a specified number of iterations. If you increase the number of iterations or adjust the hyperparameters, the Q-values might converge to more optimal values, depending on the problem and reward structure.\n"
      ],
      "metadata": {
        "id": "SDscE5n1Kxmv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dIti3AXYKtNo",
        "outputId": "728741f9-7f01-4a74-d993-58debfe6cd7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Q-values after 10 iteration(s)\n",
            "State (0,0): (-1, 0) Q=7.82\n",
            "State (0,0): (1, 0) Q=6.77\n",
            "State (0,0): (0, -1) Q=7.71\n",
            "State (0,0): (0, 1) Q=8.98\n",
            "-----------------\n",
            "State (0,1): (-1, 0) Q=10.00\n",
            "State (0,1): (1, 0) Q=10.00\n",
            "State (0,1): (0, -1) Q=10.00\n",
            "State (0,1): (0, 1) Q=10.00\n",
            "-----------------\n",
            "State (0,2): (-1, 0) Q=-10.00\n",
            "State (0,2): (1, 0) Q=-10.00\n",
            "State (0,2): (0, -1) Q=-10.00\n",
            "State (0,2): (0, 1) Q=-10.00\n",
            "-----------------\n",
            "State (1,0): (-1, 0) Q=7.90\n",
            "State (1,0): (1, 0) Q=4.64\n",
            "State (1,0): (0, -1) Q=6.34\n",
            "State (1,0): (0, 1) Q=6.46\n",
            "-----------------\n",
            "State (1,1): (-1, 0) Q=8.36\n",
            "State (1,1): (1, 0) Q=4.34\n",
            "State (1,1): (0, -1) Q=6.91\n",
            "State (1,1): (0, 1) Q=4.77\n",
            "-----------------\n",
            "State (1,2): (-1, 0) Q=-10.99\n",
            "State (1,2): (1, 0) Q=3.08\n",
            "State (1,2): (0, -1) Q=6.73\n",
            "State (1,2): (0, 1) Q=5.25\n",
            "-----------------\n",
            "State (2,0): (-1, 0) Q=6.62\n",
            "State (2,0): (1, 0) Q=2.78\n",
            "State (2,0): (0, -1) Q=5.12\n",
            "State (2,0): (0, 1) Q=4.19\n",
            "-----------------\n",
            "State (2,1): (-1, 0) Q=6.60\n",
            "State (2,1): (1, 0) Q=2.84\n",
            "State (2,1): (0, -1) Q=5.08\n",
            "State (2,1): (0, 1) Q=3.21\n",
            "-----------------\n",
            "State (2,2): (-1, 0) Q=5.14\n",
            "State (2,2): (1, 0) Q=1.70\n",
            "State (2,2): (0, -1) Q=4.91\n",
            "State (2,2): (0, 1) Q=3.68\n",
            "-----------------\n",
            "State (3,0): (-1, 0) Q=5.08\n",
            "State (3,0): (1, 0) Q=3.40\n",
            "State (3,0): (0, -1) Q=3.33\n",
            "State (3,0): (0, 1) Q=2.58\n",
            "-----------------\n",
            "State (3,1): (-1, 0) Q=4.89\n",
            "State (3,1): (1, 0) Q=3.28\n",
            "State (3,1): (0, -1) Q=3.26\n",
            "State (3,1): (0, 1) Q=1.74\n",
            "-----------------\n",
            "State (3,2): (-1, 0) Q=3.09\n",
            "State (3,2): (1, 0) Q=1.91\n",
            "State (3,2): (0, -1) Q=3.47\n",
            "State (3,2): (0, 1) Q=2.26\n",
            "-----------------\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Grid size\n",
        "n, m = 4, 3\n",
        "\n",
        "# Rewards grid setup\n",
        "rewards = np.array([\n",
        "    [-1, 10, -10],\n",
        "    [-1, -1, -1],\n",
        "    [-1, -1, -1],\n",
        "    [-1, -1, -1]\n",
        "])\n",
        "\n",
        "# Initialize Q-values to zeros (Q-table)\n",
        "q_values = np.zeros((n, m, 4))\n",
        "\n",
        "# Possible actions that the agent can take: move up, down, left, or right.\n",
        "actions = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n",
        "\n",
        "# Discount factor gamma determines the agent's consideration for future rewards.\n",
        "gamma = 1.0\n",
        "\n",
        "# Learning rate alpha for Q-learning\n",
        "alpha = 0.5\n",
        "\n",
        "# Number of iterations\n",
        "iterations = 10\n",
        "\n",
        "# Exploration rate (epsilon) for epsilon-greedy strategy\n",
        "epsilon = 0.1\n",
        "\n",
        "# Perform Q-learning with epsilon-greedy exploration\n",
        "for it in range(iterations):\n",
        "    #print(f\"Iteration {it + 1}\")\n",
        "    for i in range(n):\n",
        "        for j in range(m):\n",
        "            for action_index, action in enumerate(actions):\n",
        "                ni, nj = i + action[0], j + action[1]\n",
        "\n",
        "                # Ensure the agent stays within the grid\n",
        "                ni = max(0, min(n - 1, ni))\n",
        "                nj = max(0, min(m - 1, nj))\n",
        "\n",
        "                # If current state is (0,1) or (0,2), update the Q-values based on their respective rewards\n",
        "                if (i, j) == (0, 1):\n",
        "                    q_values[i, j, action_index] = 10\n",
        "                    continue\n",
        "                elif (i, j) == (0, 2):\n",
        "                    q_values[i, j, action_index] = -10\n",
        "                    continue\n",
        "\n",
        "                # Explore with probability epsilon or exploit with probability 1 - epsilon\n",
        "                if np.random.rand() < epsilon:\n",
        "                    action_index = np.random.randint(0, len(actions))\n",
        "                    action = actions[action_index]\n",
        "\n",
        "                # Q-learning update rule:\n",
        "                # 1. Calculate the sample using the reward and the maximum Q-value of the next state\n",
        "                sample = rewards[i, j] + gamma * np.max(q_values[ni, nj])\n",
        "\n",
        "                # 2. Update Q-value using the Q-learning update rule (weighted average)\n",
        "                q_values[i, j, action_index] = (1 - alpha) * q_values[i, j, action_index] + alpha * sample\n",
        "\n",
        "                # Clip the Q-value to a maximum of 10 for (0,1) and a minimum of -10 for (0,2)\n",
        "                if i == 0 and j == 1:\n",
        "                    q_values[i, j, action_index] = min(q_values[i, j, action_index], 10.0)\n",
        "                elif i == 0 and j == 2:\n",
        "                    q_values[i, j, action_index] = max(q_values[i, j, action_index], -10.0)\n",
        "\n",
        "# Display final Q-values and corresponding actions for all states and paths\n",
        "print(f\"Final Q-values after {iterations} iteration(s)\")\n",
        "for i in range(n):\n",
        "    for j in range(m):\n",
        "        for action_index, action in enumerate(actions):\n",
        "            print(f\"State ({i},{j}): {action} Q={q_values[i, j, action_index]:.2f}\")\n",
        "        print(\"-----------------\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bHf_Ay9pKw1a"
      }
    }
  ]
}